# doc.py
import streamlit as st

def display_documentation():
    """Exibe a documenta√ß√£o completa e formatada do projeto usando abas e colunas."""

    st.markdown("## <span class='material-symbols-outlined'>menu_book</span> Documenta√ß√£o Detalhada do Projeto", unsafe_allow_html=True)
    st.caption("Navegue pelas abas abaixo para explorar todos os aspectos do projeto, desde a configura√ß√£o at√© o uso e solu√ß√£o de problemas.")
    st.subheader(" ", divider="rainbow")

    tab_labels = [ # Emojis diretamente nos nomes das abas para melhor apelo visual
        "üåü Vis√£o Geral",
        "üõ†Ô∏è Tecnologias",
        "üñ•Ô∏è Config. Ambiente",
        "üß† Config. Modelos IA",
        "üöÄ Executando o App",
        "üß≠ Guia de Uso",
        "‚ùì FAQ/Problemas"
    ]
    tab_overview, tab_tech, tab_env_setup, tab_ia_setup, tab_running_app, tab_usage_guide, tab_faq = st.tabs(tab_labels)


    # --- Aba: Vis√£o Geral ---
    with tab_overview:
        col_ov1, col_ov2 = st.columns(2)

        with col_ov1:
            st.success("### **Prop√≥sito e Objetivos**", icon="üéØ")
            st.markdown("""
            Este projeto foi concebido como resposta √† atividade da disciplina :blue[**T√≥picos Avan√ßados em Sistemas para Internet I**] do curso :blue[**Tecnologia em Sistemas para Internet**]. Fundamentalmente, √© uma ferramenta robusta e intuitiva para **transformar conte√∫do de √°udio em texto transcrito e, subsequentemente, em insights concisos atrav√©s da extra√ß√£o de pontos-chave**.

            O objetivo principal √© oferecer uma solu√ß√£o de c√≥digo aberto que demonstre a integra√ß√£o eficaz de tecnologias de ponta em Intelig√™ncia Artificial (IA) para processamento de linguagem natural e √°udio. A aplica√ß√£o √© acess√≠vel por meio de uma interface web interativa, priorizando a facilidade de uso e a execu√ß√£o local dos modelos para maior privacidade e controle do usu√°rio.

            **P√∫blico-Alvo Principal:**
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>school</span> Estudantes e Pesquisadores:** Para transcrever aulas, palestras, entrevistas e facilitar a an√°lise qualitativa e quantitativa de material de √°udio.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>business_center</span> Profissionais de Diversas √Åreas:** Para registrar e sumarizar reuni√µes, workshops, depoimentos ou qualquer conte√∫do falado relevante para suas atividades, otimizando o tempo e a reten√ß√£o de informa√ß√£o.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>campaign</span> Criadores de Conte√∫do e Jornalistas:** Para gerar legendas, transcrever entrevistas ou resumos de podcasts, v√≠deos e outros materiais de √°udio, agilizando o fluxo de trabalho.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>developer_mode_tv</span> Desenvolvedores e Entusiastas de IA:** Como um exemplo pr√°tico e um ponto de partida para explorar a aplica√ß√£o e integra√ß√£o de modelos como Whisper e LLMs via Ollama em projetos pr√≥prios.
            """, unsafe_allow_html=True)

        with col_ov2:
            st.success("### **Principais Benef√≠cios**", icon="‚ú®")
            st.markdown("""
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>schedule</span> Economia de Tempo:** Automatiza o processo manual e frequentemente demorado de transcri√ß√£o, liberando tempo para tarefas mais estrat√©gicas.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>accessibility_new</span> Acessibilidade Aprimorada:** Torna o conte√∫do de √°udio mais acess√≠vel para pessoas com defici√™ncia auditiva e facilita a pesquisa textual dentro do material falado.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>analytics</span> Efici√™ncia na An√°lise de Dados:** A extra√ß√£o de pontos-chave permite identificar rapidamente as informa√ß√µes cruciais e os temas centrais do √°udio.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>security</span> Privacidade e Controle Local:** Ao utilizar modelos que rodam localmente (especialmente com Ollama), os dados do usu√°rio permanecem em sua m√°quina, garantindo maior privacidade e controle.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>tune</span> Customiza√ß√£o e Flexibilidade:** Oferece op√ß√µes para selecionar diferentes modelos (tamanhos do Whisper, escolha do LLM no Ollama) de acordo com a necessidade espec√≠fica de precis√£o, velocidade e recursos dispon√≠veis.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>code_off</span> Baixo Custo/Gratuito:** Utiliza ferramentas e modelos open-source, eliminando custos de APIs pagas para transcri√ß√£o e processamento de linguagem.
            """, unsafe_allow_html=True)

        st.subheader(" ", divider="rainbow")
        st.subheader("üîë Funcionalidades Detalhadas")

        # Funcionalidades em colunas, com cada feature em um container destacado
        cols_features_1, cols_features_2 = st.columns(2)

        with cols_features_1:
            with st.container(border=True):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>record_voice_over</span> Transcri√ß√£o de √Åudio Precisa", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>file_upload</span> Upload facilitado de arquivos de √°udio nos formatos `.wav` e `.mp3`.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>hearing_disabled</span> Utiliza√ß√£o do robusto modelo OpenAI Whisper para reconhecimento de fala.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>translate</span> Prioriza√ß√£o para o idioma Portugu√™s (`language='pt'`) para maior acur√°cia em conte√∫dos locais.
                """, unsafe_allow_html=True)
            st.markdown("<br>", unsafe_allow_html=True)
            with st.container(border=True):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>psychology</span> Extra√ß√£o Inteligente de Pontos-Chave", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>hub</span> Integra√ß√£o com Modelos de Linguagem Grandes (LLMs) atrav√©s do Ollama para an√°lise sem√¢ntica.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>edit_note</span> Identifica√ß√£o e apresenta√ß√£o concisa dos principais t√≥picos e informa√ß√µes da transcri√ß√£o.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>dynamic_feed</span> Flexibilidade para o usu√°rio definir qual modelo LLM (ex: `llama3`, `mistral`) localmente dispon√≠vel no Ollama ser√° utilizado.
                """, unsafe_allow_html=True)

        with cols_features_2:
            with st.container(border=True):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>tune</span> Configura√ß√£o Flex√≠vel de Modelos", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>model_training</span> Sele√ß√£o do tamanho do modelo Whisper (`tiny`, `base`, `small`, `medium`, `large`) para balancear velocidade e precis√£o.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>settings_ethernet</span> Campo para especificar o nome do modelo Ollama a ser utilizado, conforme configurado localmente pelo usu√°rio.
                """, unsafe_allow_html=True)
            st.markdown("<br>", unsafe_allow_html=True)
            with st.container(border=True):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>preview</span> Interface Clara e Feedback ao Usu√°rio", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>smart_display</span> Interface web interativa e amig√°vel constru√≠da com Streamlit.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>hourglass_top</span> Indicadores de progresso (spinners) com exibi√ß√£o de tempo decorrido durante as opera√ß√µes.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>play_circle</span> Player de √°udio integrado para verifica√ß√£o do arquivo carregado.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>notifications_active</span> Notifica√ß√µes (toasts) para informar sobre a conclus√£o de etapas.
                """, unsafe_allow_html=True)

        st.subheader(" ", divider="rainbow")
        st.subheader("üèóÔ∏è Arquitetura Simplificada do Sistema")
        st.markdown("""
        O funcionamento da aplica√ß√£o pode ser entendido atrav√©s de suas principais camadas e o fluxo de dados entre elas:
        """)

        arch_col1, arch_col2, arch_col3 = st.columns(3)
        with arch_col1:
            st.markdown("<div align='center'><strong><span class='material-symbols-outlined' style='font-size:1.5em; vertical-align:middle;'>wysiwyg</span><br>1. Frontend<br>(Interface do Usu√°rio)</strong></div>", unsafe_allow_html=True)
            st.caption("Desenvolvida com **Streamlit**. Coleta o √°udio e as configura√ß√µes do usu√°rio, e exibe os resultados finais.")
        with arch_col2:
            st.markdown("<div align='center'><strong><span class='material-symbols-outlined' style='font-size:1.5em; vertical-align:middle;'>graphic_eq</span><br>2. Transcri√ß√£o<br>(Processamento Local)</strong></div>", unsafe_allow_html=True)
            st.caption("Utiliza a biblioteca **OpenAI Whisper**. Converte o arquivo de √°udio em texto bruto diretamente na m√°quina do usu√°rio.")
        with arch_col3:
            st.markdown("<div align='center'><strong><span class='material-symbols-outlined' style='font-size:1.5em; vertical-align:middle;'>summarize</span><br>3. Pontos-Chave<br>(Processamento Local)</strong></div>", unsafe_allow_html=True)
            st.caption("Emprega **Ollama + LLM**. Recebe o texto transcrito e o processa para extrair os principais insights e pontos.")

        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("<p style='text-align:center; font-weight:bold;'><span class='material-symbols-outlined' style='vertical-align:middle; font-size:1.2em;'>flowsheet</span> Fluxo de Dados da Aplica√ß√£o:</p>", unsafe_allow_html=True)

        col_spacer1, col_chart, col_spacer2 = st.columns([0.5, 3, 0.5])

        with col_chart:
            st.graphviz_chart("""
                digraph {
                    rankdir="LR"; 

                    node [shape=box, style="rounded,filled", fillcolor="#E6F2FF", fontname="Arial", fontsize="10"];
                    edge [fontname="Arial", fontsize="9"];

                    usuario [label="üë§ Usu√°rio", shape=circle, fillcolor="#FFEBE6"];
                    streamlit_app [label="üñ•Ô∏è Interface Streamlit", fillcolor="#D1FFD1"];
                    whisper_engine [label="üéôÔ∏è Motor Whisper (Local)", fillcolor="#FFF0E6"];
                    ollama_llm [label="üß† Ollama + LLM (Local)", fillcolor="#E6E6FF"];

                    usuario -> streamlit_app [label=" Upload de √Åudio\\n+ Configura√ß√µes"];
                    streamlit_app -> whisper_engine [label="  Arquivo de √Åudio  "];
                    whisper_engine -> streamlit_app [label="  Texto Transcrito  "];
                    streamlit_app -> ollama_llm [label="  Texto Transcrito  "];
                    ollama_llm -> streamlit_app [label="  Pontos-Chave  "];
                }
            """)
            st.caption("Diagrama do fluxo de dados principal da aplica√ß√£o, da entrada do usu√°rio at√© a exibi√ß√£o dos resultados.")


    # --- Aba: Tecnologias ---
    with tab_tech:
        st.subheader("üõ†Ô∏è Stack Tecnol√≥gico Utilizado")
        st.markdown("""
        Este projeto √© o resultado da sinergia entre diversas ferramentas e bibliotecas de ponta,
        cada uma desempenhando um papel crucial na funcionalidade e experi√™ncia do usu√°rio.
        """)
        st.subheader(" ", divider="rainbow")

        col_tech1, col_tech2 = st.columns(2)
        with col_tech1:
            with st.expander("Python (Linguagem Principal)", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>code</span> Python", unsafe_allow_html=True)
                st.markdown("""
                Linguagem de programa√ß√£o vers√°til, poderosa e de alto n√≠vel, amplamente adotada em desenvolvimento web,
                ci√™ncia de dados, intelig√™ncia artificial e automa√ß√£o. Sua sintaxe clara e a vasta gama de bibliotecas
                (como PyTorch/TensorFlow, sobre os quais modelos como o Whisper s√£o constru√≠dos) o tornam ideal para este tipo de aplica√ß√£o.
                Neste projeto, Python √© a espinha dorsal que interconecta todos os componentes.
                """)

            with st.expander("OpenAI Whisper (Transcri√ß√£o de √Åudio)", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>mic_external_on</span> OpenAI Whisper", unsafe_allow_html=True)
                st.markdown("""
                Um modelo de Reconhecimento Autom√°tico de Fala (ASR) de √∫ltima gera√ß√£o, desenvolvido e treinado pela OpenAI.
                √â capaz de transcrever √°udio em m√∫ltiplos idiomas com not√°vel precis√£o, mesmo em condi√ß√µes de √°udio
                desafiadoras (ru√≠do, sotaques diversos). Sua arquitetura robusta e a disponibilidade de modelos de
                diferentes tamanhos permitem um equil√≠brio entre velocidade de processamento e acur√°cia da transcri√ß√£o.
                [Saiba mais sobre Whisper](https://openai.com/research/whisper).
                """)

            with st.expander("Requests (Comunica√ß√£o HTTP)", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>http</span> Requests", unsafe_allow_html=True)
                st.markdown("""
                Uma biblioteca Python elegante e simples para fazer requisi√ß√µes HTTP. Considerada o padr√£o de fato para
                intera√ß√µes HTTP em Python devido √† sua API amig√°vel. Neste projeto, √© essencial para permitir que
                a aplica√ß√£o Streamlit envie o texto transcrito para o servidor local do Ollama (que exp√µe uma API REST)
                e receba os pontos-chave gerados pelo LLM.
                """)

        with col_tech2:
            with st.expander("Streamlit (Interface Web)", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>dashboard_customize</span> Streamlit", unsafe_allow_html=True)
                st.markdown("""
                Um framework open-source em Python que permite transformar scripts de an√°lise de dados e modelos de IA
                em aplica√ß√µes web interativas e compartilh√°veis com um esfor√ßo m√≠nimo de codifica√ß√£o frontend.
                Sua facilidade de uso, vasta gama de widgets interativos (bot√µes, seletores, upload de arquivos, etc.)
                e capacidade de recarregamento r√°pido tornam o desenvolvimento e a prototipagem √°geis.
                [Visite Streamlit](https://streamlit.io).
                """)

            with st.expander("Ollama (LLMs Locais)", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>hub</span> Ollama", unsafe_allow_html=True)
                st.markdown("""
                Uma plataforma poderosa que simplifica drasticamente o processo de download, configura√ß√£o e execu√ß√£o
                de Modelos de Linguagem Grandes (LLMs) populares, como Llama, Mistral, Gemma, entre outros, diretamente
                na sua m√°quina local. Ollama promove a experimenta√ß√£o e o uso de IA generativa com maior privacidade,
                controle e sem a necessidade de depender de APIs externas ou custos associados. Suporta uma crescente
                biblioteca de modelos abertos. [Conhe√ßa o Ollama](https://ollama.com).
                """)

        st.subheader(" ", divider="rainbow")
        st.warning(
            """**Depend√™ncia Externa Essencial: FFmpeg**

            ‚ö†Ô∏è O `ffmpeg` √© uma su√≠te de software crucial para manipula√ß√£o de multim√≠dia. No contexto deste projeto,
            o Whisper depende do `ffmpeg` para decodificar e processar uma ampla variedade de formatos de arquivo de √°udio
            antes da transcri√ß√£o. Sua aus√™ncia ou configura√ß√£o incorreta no sistema resultar√° em falhas na
            transcri√ß√£o. As instru√ß√µes detalhadas de instala√ß√£o do `ffmpeg` podem ser encontradas na aba "Config. Ambiente".
            """,
            icon="üö®"
        )

    # --- Aba: Configura√ß√£o do Ambiente ---
    with tab_env_setup:
        st.subheader("üñ•Ô∏è Configura√ß√£o do Ambiente de Desenvolvimento/Execu√ß√£o")
        st.markdown("Siga estes passos detalhados para preparar seu ambiente local e executar a aplica√ß√£o. Uma configura√ß√£o correta √© essencial para que todas as funcionalidades operem como esperado.")
        st.subheader(" ", divider="rainbow")

        col_env1, col_env2 = st.columns(2)

        with col_env1:
            with st.expander("Passo 1: Instala√ß√£o do Python", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>code</span> 1. Python", unsafe_allow_html=True)
                st.info("‚û°Ô∏è Certifique-se de ter Python instalado. Vers√µes **3.8 a 3.11** s√£o recomendadas. Voc√™ pode baixar Python em [python.org](https://www.python.org/downloads/).", icon="üêç")
                st.markdown("""
                - Ap√≥s a instala√ß√£o, √© crucial verificar se o Python e o `pip` (o gerenciador de pacotes do Python) est√£o corretamente configurados nas vari√°veis de ambiente do seu sistema (PATH). Abra um novo terminal ou prompt de comando e execute:
                  ```bash
                  python --version
                  pip --version
                  ```
                - Se os comandos n√£o forem reconhecidos, voc√™ precisar√° adicionar manualmente os diret√≥rios de instala√ß√£o do Python e da sua subpasta `Scripts` (onde o `pip` geralmente reside no Windows) √† vari√°vel de ambiente PATH do seu sistema.
                """)

            with st.expander("Passo 2: Cria√ß√£o de Ambiente Virtual (Recomendado)", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>eco</span> 2. Ambiente Virtual", unsafe_allow_html=True)
                st.success("üí° Para evitar conflitos de depend√™ncia entre diferentes projetos Python e manter seu ambiente global limpo, √© uma **excelente pr√°tica de desenvolvimento** usar um ambiente virtual dedicado para este projeto.", icon="üõ°Ô∏è")
                st.markdown("""
                **Benef√≠cios:**
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>shield</span> Isolamento:** As bibliotecas e suas vers√µes espec√≠ficas instaladas para este projeto n√£o interferir√£o com outros projetos Python ou com a instala√ß√£o global do Python no seu sistema.
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>repeat</span> Reprodutibilidade:** Facilita a recria√ß√£o do mesmo ambiente de desenvolvimento em outras m√°quinas ou por outros colaboradores, usando um arquivo `requirements.txt`.

                **Como criar e ativar (exemplo usando `venv`):**
                No seu terminal, navegue at√© a pasta onde voc√™ deseja criar o projeto (ou onde voc√™ j√° extraiu os arquivos) e execute:
                ```bash
                # 1. Criar o ambiente virtual (ser√° criada uma pasta chamada '.venv')
                python -m venv .venv

                # 2. Ativar o ambiente virtual:
                #    No Linux/macOS:
                source .venv/bin/activate
                #    No Windows (Prompt de Comando):
                #    .venv\\Scripts\\activate.bat
                #    No Windows (PowerShell):
                #    .venv\\Scripts\\Activate.ps1
                ```
                - <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>terminal</span> *Voc√™ normalmente ver√° o nome do ambiente (ex: `(.venv)`) no in√≠cio do prompt do seu terminal, indicando que ele est√° ativo.*
                - <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>playlist_add_check</span> *Todos os comandos `pip install` para este projeto devem ser executados com o ambiente virtual ativado.*
                - <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>toggle_off</span> Para desativar o ambiente virtual quando terminar de trabalhar no projeto, simplesmente digite `deactivate` no terminal.
                """, unsafe_allow_html=True)

        with col_env2:
            with st.expander("Passo 3: Obten√ß√£o do C√≥digo Fonte do Projeto", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>folder_zip</span> 3. C√≥digo Fonte do Projeto", unsafe_allow_html=True)
                st.markdown("""
                Voc√™ precisar√° dos arquivos da aplica√ß√£o (`app.py`, `doc.py`, etc.) para execut√°-la.
                - **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>archive</span> Se voc√™ recebeu os arquivos como um `.zip`:** Extraia o conte√∫do para uma pasta de sua prefer√™ncia no seu computador.
                - **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>hub</span> Se for um reposit√≥rio Git (exemplo):** Use o Git para clonar o reposit√≥rio (substitua a URL pelo link correto, se aplic√°vel).
                  ```bash
                  git clone [https://github.com/SEU_USUARIO/SEU_REPOSITORIO.git](https://github.com/SEU_USUARIO/SEU_REPOSITORIO.git)
                  cd NOME_DO_REPOSITORIO_CLONADO
                  ```
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>snippet_folder</span> A estrutura b√°sica de arquivos que voc√™ deve ter na pasta do projeto inclui, no m√≠nimo:
                    - `app.py` (o script principal da aplica√ß√£o Streamlit)
                    - `doc.py` (este arquivo de documenta√ß√£o)
                    - `requirements.txt` (arquivo de depend√™ncias Python, veja o pr√≥ximo passo)
                    - Opcionalmente, `config.py` (se usado para configura√ß√µes como o rodap√©).
                """, unsafe_allow_html=True)

            with st.expander("Passo 4: Instala√ß√£o das Depend√™ncias Python", expanded=False):
                st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>list_alt</span> 4. Instala√ß√£o das Depend√™ncias Python", unsafe_allow_html=True)
                st.markdown("""
                As bibliotecas Python externas necess√°rias para o funcionamento do projeto est√£o listadas em um arquivo `requirements.txt`.

                1.  **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>description</span> Crie o arquivo `requirements.txt`** na raiz do seu projeto com o seguinte conte√∫do:
                    ```text
                    - streamlit
                    - openai-whisper
                    - requests
                    ```
                2.  **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>download</span> Instale as depend√™ncias:** Com o ambiente virtual (se voc√™ criou um) **ativado**, execute o seguinte comando no terminal:
                    ```bash
                    pip install -r requirements.txt
                    ```
                Isso instalar√° as seguintes bibliotecas e suas respectivas depend√™ncias:
                - **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>rocket_launch</span> Streamlit:** Framework utilizado para construir e servir a interface web interativa da aplica√ß√£o.
                - **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>mic</span> OpenAI Whisper:** Biblioteca da OpenAI para realizar a transcri√ß√£o de √°udio para texto de alta precis√£o.
                - **<span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>http</span> Requests:** Biblioteca para realizar chamadas HTTP, usada  para comunicar com a API local do servidor Ollama.
                """, unsafe_allow_html=True)
                st.caption("Nota: A depend√™ncia `ffmpeg`, que √© crucial para o Whisper, √© uma instala√ß√£o a n√≠vel de sistema e √© tratada no pr√≥ximo passo.")

        st.subheader(" ", divider="rainbow")

        with st.expander("Passo 5: Instala√ß√£o do FFmpeg (Depend√™ncia Cr√≠tica para Whisper)", expanded=False):
            st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>movie_filter</span> 5. FFmpeg", unsafe_allow_html=True)
            st.error("üõë **FFmpeg √© absolutamente essencial** para que a biblioteca Whisper possa processar arquivos de √°udio corretamente. \n\nSem o FFmpeg instalado e devidamente configurado no PATH do seu sistema, a funcionalidade de transcri√ß√£o provavelmente falhar√° ou n√£o suportar√° todos os formatos de √°udio.", icon="‚ö†Ô∏è")

            st.markdown("Instale o FFmpeg no seu sistema operacional:")

            col_ffmpeg_linux, col_ffmpeg_mac, col_ffmpeg_win = st.columns(3)

            with col_ffmpeg_linux:
                st.markdown("##### <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>terminal</span> Linux (Debian/Ubuntu)", unsafe_allow_html=True)
                st.markdown("""
                Execute no terminal:
                ```bash
                sudo apt update && sudo apt install ffmpeg
                ```
                """, unsafe_allow_html=True)

            with col_ffmpeg_mac:
                st.markdown("##### <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>desktop_mac</span> macOS (Homebrew)", unsafe_allow_html=True)
                st.markdown("""
                Se voc√™ usa o [Homebrew](https://brew.sh/index_pt-br):
                ```bash
                brew install ffmpeg
                ```
                """, unsafe_allow_html=True)

            with col_ffmpeg_win:
                st.markdown("##### <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>desktop_windows</span> Windows", unsafe_allow_html=True)
                st.markdown("""
                1.  <span class='material-symbols-outlined' style='font-size:0.9em; vertical-align:middle;'>download_for_offline</span> Baixe os bin√°rios (ex: de [Gyan.dev](https://www.gyan.dev/ffmpeg/builds/)).
                2.  <span class='material-symbols-outlined' style='font-size:0.9em; vertical-align:middle;'>folder_open</span> Extraia para uma pasta (ex: `C:\\ffmpeg`).
                3.  <span class='material-symbols-outlined' style='font-size:0.9em; vertical-align:middle;'>rule_folder</span> Adicione a pasta `bin` (ex: `C:\\ffmpeg\\bin`) ao PATH do sistema.
                4.  <span class='material-symbols-outlined' style='font-size:0.9em; vertical-align:middle;'>restart_alt</span> **Reinicie** terminais abertos ou o sistema.
                """, unsafe_allow_html=True)

            st.subheader(" ", divider="rainbow")
            st.markdown("""
            **<span class='material-symbols-outlined' style='vertical-align:middle;'>task_alt</span> Verificando a Instala√ß√£o do FFmpeg:**
            Para confirmar, abra um **novo** terminal e digite:
            ```bash
            ffmpeg -version
            ```
            Voc√™ dever√° ver informa√ß√µes sobre a vers√£o do FFmpeg. Se ocorrer um erro de "comando n√£o encontrado", revise a instala√ß√£o e a configura√ß√£o do PATH.
            """, unsafe_allow_html=True)

    # --- Aba: Configura√ß√£o dos Modelos IA ---
    with tab_ia_setup:
        st.subheader("üß† Configura√ß√£o dos Modelos de Intelig√™ncia Artificial")
        st.markdown("""
        Esta se√ß√£o detalha como configurar os modelos de IA necess√°rios para as funcionalidades de transcri√ß√£o (Whisper)
        e extra√ß√£o de pontos-chave (Ollama com LLMs). Uma configura√ß√£o correta √© crucial para o funcionamento do aplicativo.
        """)
        st.subheader(" ", divider="rainbow")

        col_whisper, col_ollama_setup = st.columns(2) # Renomeado para evitar conflito

        with col_whisper:
            st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>hearing</span> 1. Modelos Whisper (Transcri√ß√£o)", unsafe_allow_html=True)

            with st.expander("Detalhes e Considera√ß√µes sobre os Modelos Whisper", expanded=False):
                st.markdown("""
                Os modelos de transcri√ß√£o do OpenAI Whisper s√£o o cora√ß√£o da funcionalidade de convers√£o de √°udio para texto.
                Eles s√£o gerenciados pela pr√≥pria biblioteca `openai-whisper`.

                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>file_download</span> Download Autom√°tico:** Ao selecionar um tamanho de modelo na interface do aplicativo (`tiny`, `base`, `small`, `medium`, `large`) pela primeira vez, o modelo correspondente ser√° baixado automaticamente.
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>wifi</span> Requisito de Internet:** √â necess√°ria uma conex√£o com a internet para este download inicial de cada modelo.
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>folder_managed</span> Local de Cache:** Os modelos baixados s√£o armazenados localmente para uso futuro, geralmente na pasta `~/.cache/whisper` (no diret√≥rio home do seu usu√°rio, dentro da pasta `.cache`).
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>tune</span> Considera√ß√µes sobre os Modelos:**
                    - Modelos menores (ex: `tiny`, `base`, `small`) s√£o **mais r√°pidos** e exigem **menos recursos** computacionais (RAM/VRAM), mas podem ser **menos precisos** na transcri√ß√£o.
                    - Modelos maiores (ex: `medium`, `large`) oferecem **maior precis√£o**, mas s√£o **significativamente mais lentos** e consomem **mais recursos**. O modelo `large` pode ser particularmente pesado para CPUs e se beneficia enormemente de uma GPU NVIDIA com suporte a CUDA.
                    - **Desempenho:** Os modelos s√£o otimizados para rodar em CPU, mas o desempenho, especialmente dos modelos `medium` e `large`, √© substancialmente melhor com uma GPU NVIDIA compat√≠vel.
                """, unsafe_allow_html=True)
                st.caption("O aplicativo est√° configurado para priorizar transcri√ß√µes em Portugu√™s (`language='pt'`).")

        with col_ollama_setup: # Usando o nome renomeado
            st.markdown("#### <span class='material-symbols-outlined' style='vertical-align:middle;'>hub</span> 2. Ollama e LLMs (Pontos-Chave)", unsafe_allow_html=True)
            st.markdown("""
            Ollama √© a plataforma utilizada para executar Modelos de Linguagem Grandes (LLMs) localmente em sua m√°quina.
            Estes LLMs analisam o texto transcrito para extrair os pontos-chave. <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>psychology</span>
            """, unsafe_allow_html=True)

            with st.expander("Etapa 2.1: Instala√ß√£o do Ollama", expanded=False):
                st.markdown("##### <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>system_update_alt</span> Guia de Instala√ß√£o", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>link</span> Visite o [site oficial do Ollama](https://ollama.com/download) e baixe o instalador para seu sistema operacional (Windows, macOS, Linux).
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>desktop_windows</span> Siga as instru√ß√µes de instala√ß√£o fornecidas.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>terminal</span> Ap√≥s a instala√ß√£o, abra um **novo terminal** e execute `ollama --version` para confirmar.
                """, unsafe_allow_html=True)

            with st.expander("Etapa 2.2: Download de Modelos LLM via Ollama", expanded=False):
                st.markdown("##### <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>model_training</span> Baixando os Modelos Necess√°rios", unsafe_allow_html=True)
                st.markdown("""
                Antes de usar um LLM com o aplicativo, voc√™ precisa baix√°-lo usando o Ollama. O aplicativo sugere `llama3` por padr√£o.
                <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>terminal</span> Abra seu terminal e execute:
                ```bash
                # Para baixar o Llama 3 (modelo 8B, ~4.7GB, bom equil√≠brio)
                ollama pull llama3

                # Para baixar o Mistral (outro modelo popular, ~4.1GB)
                ollama pull mistral

                # Para um modelo menor e mais r√°pido (ex: Qwen2 0.5B, ~0.4GB)
                ollama pull qwen2:0.5b
                ```
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>pageview</span> Explore mais modelos na [biblioteca Ollama](https://ollama.com/library).
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>badge</span> O nome usado no comando `pull` (ex: `llama3`) √© o que voc√™ deve inserir na interface do app.
                - **Tamanhos dos Modelos:** LLMs v√™m em diversos tamanhos (ex: 0.5B, 7B, 13B, 70B par√¢metros). Modelos maiores s√£o geralmente mais capazes, mas exigem mais RAM e poder de processamento. Verifique os requisitos.
                - **Atualizar um Modelo:** Para obter a vers√£o mais recente de um modelo: `ollama pull nome_do_modelo` (ou `nome_do_modelo:latest`).
                """, unsafe_allow_html=True)

            with st.expander("Etapa 2.3: Executando o Servidor Ollama", expanded=False):
                st.markdown("##### <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>dns</span> Execu√ß√£o e Verifica√ß√£o do Servidor", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>power</span> **Servi√ßo em Background (Padr√£o):** Em muitas instala√ß√µes, o Ollama inicia automaticamente.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>play_circle</span> **In√≠cio Manual / Ver Logs:** Se precisar iniciar manualmente ou quiser ver os logs em tempo real (√∫til para depura√ß√£o), abra um terminal e execute:
                  ```bash
                  ollama serve
                  ```
                  Se executar `ollama serve` em primeiro plano no terminal, voc√™ ver√° os logs de atividade do servidor diretamente.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>lan</span> **Verifica√ß√£o da Porta:** O servidor opera por padr√£o em `http://localhost:11434`.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>list_alt</span> **Listar Modelos Baixados:** Para ver os modelos que voc√™ tem localmente:
                  ```bash
                  ollama list
                  ```
                """, unsafe_allow_html=True)

        st.subheader(" ", divider="rainbow")
        st.warning(
            """
            **Cr√≠tico: Ollama e Modelos LLM**

            O servidor Ollama deve estar em execu√ß√£o, e o modelo LLM que voc√™ especificar na interface do aplicativo precisa ter sido previamente baixado com `ollama pull` e estar listado em `ollama list` para que a funcionalidade de extra√ß√£o de pontos-chave funcione corretamente.
            - :red[‚ÄºÔ∏è **Caso contr√°rio, voc√™ encontrar√° erros de comunica√ß√£o ou o campo de pontos-chave ficar√° vazio.**]
            """,
            icon="üö®"
        )

    # --- Aba: Executando a Aplica√ß√£o ---
    with tab_running_app:
        st.subheader("üöÄ Executando a Aplica√ß√£o Streamlit")
        st.markdown("""
        Com todos os pr√©-requisitos e configura√ß√µes do ambiente e dos modelos de IA conclu√≠dos (detalhados nas abas anteriores), voc√™ est√° pronto para iniciar o aplicativo.
        """)
        st.subheader(" ", divider="rainbow")

        col_checklist, col_steps_run = st.columns([0.8, 1.2]) # Renomeado col_steps para col_steps_run

        with col_checklist:
            with st.expander("üìù Checklist R√°pido Antes de Executar", expanded=True):
                st.markdown("#### <span class='material-symbols-outlined'>checklist</span> Itens Essenciais:", unsafe_allow_html=True)
                st.markdown("""
                - <span class='material-symbols-outlined' style='vertical-align:middle; font-size:1.1em;'>toggle_on</span> Ambiente virtual **ativado**?
                - <span class='material-symbols-outlined' style='vertical-align:middle; font-size:1.1em;'>dns</span> Servidor Ollama **rodando**? (Veja detalhes abaixo)
                - <span class='material-symbols-outlined' style='vertical-align:middle; font-size:1.1em;'>download_done</span> Modelo LLM desejado **baixado** via Ollama? (Veja detalhes abaixo e na aba "Config. Modelos IA")
                - <span class='material-symbols-outlined' style='vertical-align:middle; font-size:1.1em;'>folder_zip</span> Voc√™ est√° no **diret√≥rio correto** do projeto no terminal?
                - <span class='material-symbols-outlined' style='vertical-align:middle; font-size:1.1em;'>movie_filter</span> `ffmpeg` **instalado** e no PATH (ver aba "Config. Ambiente")?
                """, unsafe_allow_html=True)

            with st.expander("‚öôÔ∏è Gerenciando e Verificando o Ollama", expanded=False):
                st.markdown("##### <span class='material-symbols-outlined' style='vertical-align:middle;'>task_alt</span> Verificando se o Ollama est√° Rodando", unsafe_allow_html=True)
                st.markdown("""
                - Normalmente, o Ollama √© executado como um servi√ßo em segundo plano ap√≥s a instala√ß√£o.
                - Para confirmar, tente listar os modelos:
                  ```bash
                  ollama list
                  ```
                - Se funcionar, o servidor est√° ativo.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>desktop_windows</span> Procure pelo √≠cone do Ollama na bandeja do sistema.
                - <span class='material-symbols-outlined' style='font-size:1em; vertical-align:middle;'>lan</span> Porta padr√£o: `http://localhost:11434`.
                """, unsafe_allow_html=True)

                st.markdown("##### <span class='material-symbols-outlined' style='vertical-align:middle;'>format_list_bulleted</span> Listando Modelos Ollama Baixados", unsafe_allow_html=True)
                st.markdown("""
                - Para ver modelos dispon√≠veis localmente:
                  ```bash
                  ollama list
                  ```
                - O nome listado (ex: `llama3`) √© o que voc√™ usar√° na aplica√ß√£o.
                """, unsafe_allow_html=True)

                st.markdown("##### <span class='material-symbols-outlined' style='vertical-align:middle;'>play_circle</span> Iniciando o Servidor Ollama Manualmente / Ver Logs", unsafe_allow_html=True)
                st.markdown("""
                - Se n√£o estiver rodando ou para ver logs:
                  ```bash
                  ollama serve
                  ```
                - Se j√° estiver rodando como servi√ßo, este comando dar√° erro de porta em uso.
                - Para parar o servidor manual: `Ctrl + C` no terminal.
                """, unsafe_allow_html=True)

                st.markdown("##### <span class='material-symbols-outlined' style='vertical-align:middle;'>download</span> Baixando Novos Modelos LLM com Ollama", unsafe_allow_html=True)
                st.markdown("""
                - Modeos LLM devem ser baixados antes do uso:
                  ```bash
                  ollama pull llama3
                  ollama pull mistral
                  ollama pull qwen2:0.5b 
                  ```
                - Mais modelos na [biblioteca Ollama](https://ollama.com/library).
                - Detalhes na aba "**üß† Config. Modelos IA**".
                """, unsafe_allow_html=True)

                st.markdown("##### <span class='material-symbols-outlined' style='vertical-align:middle;'>rocket</span> Testando um Modelo Ollama Diretamente (Ex: Llama 3)", unsafe_allow_html=True)
                st.markdown("""
                - Ap√≥s confirmar que o servidor Ollama est√° rodando e o modelo (`llama3`, por exemplo) est√° listado em `ollama list`, voc√™ pode test√°-lo diretamente no terminal:
                  ```bash
                  ollama run llama3
                  ```
                - Isso iniciar√° uma sess√£o de chat interativa com o modelo no seu terminal.
                - Voc√™ pode fazer perguntas ou dar instru√ß√µes diretamente.
                - Para sair da sess√£o de chat do Ollama no terminal, digite `Ctrl + D` ou `/bye` e pressione Enter.
                - Este √© um bom teste para verificar se um modelo espec√≠fico est√° operacional.
                """, unsafe_allow_html=True)


            with st.expander("‚ÑπÔ∏è Download de Modelos Whisper", expanded=False):
                st.markdown("##### <span class='material-symbols-outlined' style='vertical-align:middle;'>system_update_alt</span> Informa√ß√µes sobre Modelos Whisper", unsafe_allow_html=True)
                st.caption("Lembre-se: os modelos Whisper s√£o baixados automaticamente pelo aplicativo no primeiro uso de cada tamanho. Uma conex√£o com a internet ser√° necess√°ria nesse momento e o download pode levar alguns minutos dependendo do tamanho do modelo e da velocidade da sua conex√£o.")


        with col_steps_run: # Usando o nome renomeado
            with st.expander("üõ†Ô∏è Passos Detalhados para Executar a Aplica√ß√£o", expanded=True):
                st.markdown(r"""
                1.  **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>power_settings_new</span> Ative o Ambiente Virtual:**
                    Se voc√™ criou um (ex: `.venv`), certifique-se de que ele est√° ativo no seu terminal atual.
                    ```bash
                    # Exemplo para Linux/macOS:
                    source .venv/bin/activate
                    # Exemplo para Windows (PowerShell):
                    # .\.venv\Scripts\Activate.ps1
                    ```

                2.  **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>fact_check</span> Verifique o Servidor Ollama e Modelos:**
                    Confirme que o servidor Ollama est√° em execu√ß√£o e que o modelo LLM desejado est√° baixado (veja o expander "‚öôÔ∏è Gerenciando e Verificando o Ollama" ao lado ou a aba "Config. Modelos IA").
                3.  **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>folder_open</span> Navegue at√© a Pasta do Projeto:**
                    No seu terminal, use o comando `cd` para ir at√© o diret√≥rio onde o arquivo `app.py` est√° localizado.
                    ```bash
                    cd caminho/para/seu/projeto
                    ```
                4.  **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>terminal</span> Execute o Comando Streamlit:**
                    Digite o seguinte comando e pressione Enter:
                    ```bash
                    streamlit run app.py
                    ```
                    ou, para potencialmente evitar problemas com o observador de arquivos:
                    ```bash
                    streamlit run app.py --server.fileWatcherType none
                    ```
                    Este comando inicia o servidor web do Streamlit.
                """, unsafe_allow_html=True)

        st.subheader(" ", divider="rainbow")

        with st.expander("üéâ O Que Esperar Ap√≥s Iniciar", expanded=True):
            st.success("Aplica√ß√£o Iniciada! O que esperar:", icon="‚úÖ")
            st.markdown("""
            - Seu navegador web padr√£o dever√° abrir uma nova aba com a interface do aplicativo.
            - Se n√£o abrir automaticamente, o terminal (onde voc√™ executou `streamlit run app.py`) mostrar√° as URLs para acesso:
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>computer</span> Local URL:** Geralmente `http://localhost:8501`
                - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>wifi</span> Network URL:** Algo como `http://SEU_ENDERECO_IP_LOCAL:8501` (√∫til para acessar de outros dispositivos na mesma rede).
            - <span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>hourglass_empty</span> **Primeiro Carregamento:** Aguarde alguns instantes para a interface carregar completamente. O primeiro uso de um modelo Whisper ou o carregamento inicial de um modelo Ollama podem levar um pouco mais de tempo.
            """, unsafe_allow_html=True)

        with st.expander("üõë Como Parar a Aplica√ß√£o", expanded=False):
            st.markdown("#### <span class='material-symbols-outlined'>stop_circle</span> Parando a Aplica√ß√£o Streamlit", unsafe_allow_html=True)
            st.markdown("""
            Para interromper o servidor Streamlit e fechar a aplica√ß√£o:
            - Volte para a janela do terminal onde voc√™ executou o comando `streamlit run app.py`.
            - Pressione as teclas `Ctrl` + `C`.
            - O terminal pode pedir uma confirma√ß√£o para encerrar o job; se sim, confirme (geralmente digitando `y` ou `s`).
            """, unsafe_allow_html=True)

        with st.expander("üí° Dicas Adicionais ao Executar", expanded=False):
            st.markdown("#### <span class='material-symbols-outlined'>tips_and_updates</span> Dicas √öteis", unsafe_allow_html=True)
            st.markdown("""
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>refresh</span> Recarregar Altera√ß√µes no C√≥digo:** Se voc√™ editar os arquivos Python (`app.py`, `doc.py`, etc.) enquanto o Streamlit est√° rodando, a interface do Streamlit no navegador geralmente mostrar√° uma op√ß√£o "Rerun" no canto superior direito, ou pode tentar recarregar automaticamente.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>important_devices</span> Acesso em Outros Dispositivos:** A "Network URL" exibida no terminal √© √∫til se voc√™ quiser testar ou usar a aplica√ß√£o em um tablet, smartphone ou outro computador que esteja conectado √† mesma rede Wi-Fi/LAN.
            - **<span class='material-symbols-outlined' style='font-size:1.1em; vertical-align:middle;'>troubleshoot</span> Observando o Terminal:** Mantenha o terminal onde o Streamlit est√° rodando vis√≠vel. Ele exibir√° informa√ß√µes √∫teis, logs de acesso e, mais importante, mensagens de erro detalhadas caso algo n√£o funcione como esperado.
            """, unsafe_allow_html=True)


    # --- Aba: Guia de Uso ---
    with tab_usage_guide:
        st.subheader("üß≠ Guia de Uso da Interface do Aplicativo")
        st.markdown("A interface foi projetada para ser intuitiva, este √© um detalhamento das principais se√ß√µes e funcionalidades:")
        st.subheader(" ", divider="rainbow")

        col_usage1, col_usage2 = st.columns(2)

        with col_usage1:
            with st.expander("Barra Lateral de Configura√ß√µes (Sidebar)", expanded=True):
                st.markdown("#### <span class='material-symbols-outlined'>view_sidebar</span> 1. Configura√ß√µes Principais", unsafe_allow_html=True)
                st.markdown("""
                Localizada √† esquerda da tela (sidebar), cont√©m todas as op√ß√µes de configura√ß√£o antes do processamento:
                - **<span class='material-symbols-outlined'>mic</span> Selecione o Modelo Whisper:**
                    - Um menu suspenso permite escolher o tamanho do modelo Whisper a ser usado para a transcri√ß√£o (ex: `tiny`, `small`, `medium`).
                    - **Impacto:** Modelos menores s√£o mais r√°pidos mas menos precisos. Modelos maiores s√£o mais precisos mas mais lentos e exigem mais recursos.
                - **<span class='material-symbols-outlined'>hub</span> Nome do Modelo Ollama:**
                    - Um campo de texto onde voc√™ deve inserir o nome exato do modelo LLM que voc√™ baixou via Ollama e deseja usar para extrair os pontos-chave (ex: `llama3`, `mistral`).
                    - Este nome deve corresponder a um modelo listado por `ollama list`.
                - **<span class='material-symbols-outlined'>upload_file</span> Escolha um arquivo de √°udio:**
                    - Bot√£o para fazer upload de um arquivo de √°udio do seu computador.
                    - **Formatos Suportados:** `.wav` e `.mp3`.
                """, unsafe_allow_html=True)

        with col_usage2:
            with st.expander("√Årea Principal de Intera√ß√£o e Resultados", expanded=True):
                st.markdown("#### <span class='material-symbols-outlined'>web_asset</span> 2. Intera√ß√£o e Exibi√ß√£o", unsafe_allow_html=True)
                st.markdown("""
                - **<span class='material-symbols-outlined'>play_circle</span> Player de √Åudio:**
                    - Ap√≥s o upload, um player de √°udio aparecer√°, permitindo que voc√™ ou√ßa o √°udio carregado.
                - **<span class='material-symbols-outlined'>rocket_launch</span> Bot√£o "Processar √Åudio":**
                    - Ao clicar nele, o processo de transcri√ß√£o e extra√ß√£o de pontos-chave √© iniciado.
                - **<span class='material-symbols-outlined'>hourglass_top</span> Indicadores de Progresso:**
                    - Durante o processamento, spinners indicar√£o as etapas em execu√ß√£o e o tempo decorrido.
                - **<span class='material-symbols-outlined'>view_column</span> Resultados em Colunas:**
                    - **Coluna da Transcri√ß√£o:** √Ä esquerda, o texto completo transcrito.
                    - **Coluna de Pontos-Chave:** √Ä direita, os pontos-chave identificados.
                - **<span class='material-symbols-outlined'>notifications</span> Notifica√ß√µes:**
                    - Mensagens curtas para indicar a conclus√£o de etapas.
                """, unsafe_allow_html=True)

        st.subheader(" ", divider="rainbow")
        with st.expander("Acesso √† Documenta√ß√£o Completa", expanded=False):
            st.markdown("#### <span class='material-symbols-outlined'>menu_book</span> 3. Acesso √† Documenta√ß√£o", unsafe_allow_html=True)
            st.markdown("""
            - No topo da p√°gina principal ou da p√°gina de documenta√ß√£o, voc√™ encontrar√° o bot√£o **"<span class='material-symbols-outlined'>visibility</span> Ver Documenta√ß√£o Interativa"** (ou similar).
            - Clicar neste bot√£o alterna a visualiza√ß√£o entre a interface principal do aplicativo e esta p√°gina de documenta√ß√£o detalhada que voc√™ est√° vendo.
            """, unsafe_allow_html=True)


    # --- Aba: FAQ/Solu√ß√£o de Problemas ---
    with tab_faq:
        st.subheader("‚ùì Perguntas Frequentes e Solu√ß√£o de Problemas")
        st.error("#### Encontrou um problema? Veja se estas dicas ajudam!", icon="üõ†Ô∏è")
        st.subheader(" ", divider="rainbow")

        col_faq1, col_faq2 = st.columns(2)

        with col_faq1:
            with st.expander("Problema: A aplica√ß√£o n√£o inicia ou mostra erro ao executar `streamlit run app.py`", expanded=False):
                st.markdown("""
                - **Verifique a instala√ß√£o do Streamlit:** `pip show streamlit`.
                - **Nome do arquivo:** Confirme que est√° no diret√≥rio correto e que o nome √© `app.py`.
                - **Ambiente Virtual:** Garanta que est√° ativado.
                - **Conflitos de Porta:** Verifique as mensagens no terminal. (Ex: `Port 8501 is already in use`).
                - **Logs do Terminal:** Observe atentamente para mensagens de erro detalhadas que indiquem o problema.
                """)

            with st.expander("Problema: Transcri√ß√£o com Whisper est√° muito lenta", expanded=False):
                st.markdown("""
                - **Modelo Grande Selecionado:** Modelos Whisper maiores (`medium`, `large`) s√£o inerentemente mais lentos, especialmente em CPUs.
                - **Recursos da M√°quina:** M√°quinas com CPU mais antiga, pouca RAM ou sem GPU dedicada (para modelos maiores) ter√£o desempenho inferior.
                - **Primeira Execu√ß√£o de um Modelo:** O download e carregamento inicial do modelo podem adicionar tempo.
                - **Solu√ß√µes:**
                    - Tente um modelo Whisper menor (ex: `base`, `small`) na barra lateral.
                    - Certifique-se de que n√£o h√° outros processos pesados consumindo recursos da sua m√°quina.
                    - Se tiver GPU NVIDIA, verifique se o PyTorch est√° configurado para us√°-la (geralmente autom√°tico se instalado corretamente).
                """)

            with st.expander("Problema: Ollama n√£o responde, erro de comunica√ß√£o, ou nenhum ponto-chave √© extra√≠do", expanded=False):
                st.markdown("""
                - **Servidor Ollama Inativo:**
                    - Verifique se o servidor Ollama est√° rodando (veja aba "Executando o App" -> "Gerenciando e Verificando o Ollama").
                    - Tente executar `ollama list` no terminal. Se der erro, o servidor n√£o est√° acess√≠vel.
                    - Se necess√°rio, inicie-o com `ollama serve` (se n√£o estiver configurado para iniciar automaticamente).
                - **Modelo LLM n√£o Baixado:**
                    - Certifique-se de que o nome do modelo LLM inserido na interface da aplica√ß√£o (ex: `llama3`) foi previamente baixado. Use `ollama pull NOME_DO_MODELO` no terminal.
                - **Nome do Modelo Incorreto na Interface:** O nome deve ser exato como aparece em `ollama list`.
                - **URL/Porta do Ollama:** O padr√£o √© `http://localhost:11434`. Se voc√™ configurou o Ollama para uma porta diferente e a aplica√ß√£o n√£o foi ajustada, haver√° falha.
                - **Recursos Insuficientes para o LLM:** Modelos LLM, mesmo rodando localmente, podem exigir bastante RAM. Se sua m√°quina tiver pouca RAM, o modelo pode n√£o carregar ou responder. Verifique os logs do `ollama serve` para mensagens de erro (`out of memory`, etc.).
                - **Firewall:** Verifique se o firewall n√£o est√° bloqueando a comunica√ß√£o local na porta 11434.
                """)

        with col_faq2:
            with st.expander("Problema: FFmpeg n√£o encontrado / Erro na transcri√ß√£o relacionado a formato de √°udio", expanded=False):
                st.markdown("""
                - **Causa Principal:** FFmpeg n√£o est√° instalado ou o diret√≥rio `bin` do FFmpeg n√£o est√° configurado na vari√°vel de ambiente PATH do seu sistema.
                - **Solu√ß√£o:**
                    1. Siga rigorosamente as instru√ß√µes de instala√ß√£o do FFmpeg para o seu sistema operacional, detalhadas na aba "**üñ•Ô∏è Config. Ambiente**".
                    2. Ap√≥s a instala√ß√£o e configura√ß√£o do PATH, **reinicie** todos os terminais abertos ou, para garantir, reinicie o computador.
                    3. Verifique a instala√ß√£o abrindo um **novo** terminal e digitando `ffmpeg -version`. Voc√™ deve ver informa√ß√µes da vers√£o, n√£o um erro.
                """)

            with st.expander("Problema: Erro ao carregar modelo Whisper / Download do modelo Whisper falha", expanded=False):
                st.markdown("""
                - **Conex√£o com a Internet:** √â necess√°ria uma conex√£o est√°vel com a internet para o download autom√°tico do modelo Whisper na primeira vez que ele √© usado.
                - **Espa√ßo em Disco Insuficiente:** Os modelos Whisper s√£o salvos em `~/.cache/whisper`. Verifique se h√° espa√ßo suficiente no disco onde seu diret√≥rio home est√° localizado.
                - **Firewall ou Proxy:** Configura√ß√µes de firewall ou proxy na sua rede podem estar bloqueando o download dos arquivos do modelo.
                - **Cache Corrompido:** Em casos raros, o cache pode estar corrompido. Voc√™ pode tentar limpar (ou renomear) a pasta `~/.cache/whisper` e deixar o aplicativo tentar baixar o modelo novamente.
                """)

            with st.expander("Problema: Qualidade da transcri√ß√£o n√£o √© ideal ou o idioma est√° incorreto", expanded=False):
                st.markdown("""
                - **Qualidade do √Åudio Original:** Ru√≠do de fundo excessivo, volume muito baixo, falantes distantes do microfone, ou m√∫ltiplos falantes sobrepostos podem degradar significativamente a precis√£o da transcri√ß√£o.
                - **Tamanho do Modelo Whisper:** Modelos Whisper menores (ex: `tiny`, `base`) s√£o mais r√°pidos, mas geralmente menos precisos que os maiores (`medium`, `large`). Considere usar um modelo maior se a precis√£o for cr√≠tica e seus recursos permitirem.
                - **Configura√ß√£o de Idioma:** Este aplicativo est√° configurado para priorizar o Portugu√™s (`language='pt'`) para o Whisper. Se o √°udio estiver em outro idioma, a transcri√ß√£o pode n√£o ser ideal ou pode tentar transcrever como se fosse Portugu√™s. Para outros idiomas, o c√≥digo precisaria ser ajustado.
                """)

        st.subheader(" ", divider="rainbow")
        st.info("‚ÑπÔ∏è **Dica Geral para Solu√ß√£o de Problemas:** Sempre verifique as mensagens de texto (logs) no terminal onde voc√™ executou `streamlit run app.py`. Se estiver usando `ollama serve` em um terminal separado, monitore esse terminal tamb√©m. Esses logs frequentemente cont√™m mensagens de erro detalhadas que s√£o cruciais para diagnosticar a causa raiz de um problema.", icon="üîç")

# Para testar este arquivo de documenta√ß√£o isoladamente, descomente as linhas abaixo:
# if __name__ == "__main__":
#     # Configura√ß√£o b√°sica da p√°gina para teste
#     st.set_page_config(
#         page_title="Documenta√ß√£o do Projeto de Transcri√ß√£o",
#         page_icon="üéôÔ∏è",
#         layout="wide",
#         initial_sidebar_state="expanded"
#     )
#     display_documentation()